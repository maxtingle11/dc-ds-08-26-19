{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting and Pipelines\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understanding how we can create a streamline of procedures (Pipelines)\n",
    "\n",
    "- What is the use of such practices.\n",
    "\n",
    "- Boosting methods - Specifically Gradient and Adaboost\n",
    "\n",
    "- Implementation of GradientBoostClassifier with fine-tuning with gridsearch.\n",
    "\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "__Q:__ What is a pipeline?\n",
    "\n",
    "[sklearn - documentation](https://scikit-learn.org/stable/modules/compose.html#pipeline)\n",
    "\n",
    "> Transformers (scaling, preprocessing, feature selection etc.) are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a Pipeline.\n",
    "\n",
    "__Q__: Why should we use pipelines?\n",
    "\n",
    "    - Convenience: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "    - Joint parameter selection: You can grid search over parameters of all estimators in the pipeline at once.\n",
    "    - Safety: Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "    \n",
    "    - standardizes procedure\n",
    "    - helps prevent data leakage\n",
    "    - \n",
    "    \n",
    "pipeline = tool\n",
    "\n",
    "boosting = idea with tools\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:18:02.782803Z",
     "start_time": "2019-10-30T20:18:01.157506Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:18:02.801735Z",
     "start_time": "2019-10-30T20:18:02.784805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the dataset \n",
    "## Source: https://www.kaggle.com/uciml/pima-indians-diabetes-database/download\n",
    "df = pd.read_csv('data/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:18:03.881237Z",
     "start_time": "2019-10-30T20:18:03.844682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>768.0</td>\n",
       "      <td>3.845052</td>\n",
       "      <td>3.369578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>768.0</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.00000</td>\n",
       "      <td>117.0000</td>\n",
       "      <td>140.25000</td>\n",
       "      <td>199.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>768.0</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>0.000</td>\n",
       "      <td>62.00000</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>122.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>768.0</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>768.0</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>127.25000</td>\n",
       "      <td>846.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>768.0</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.30000</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>36.60000</td>\n",
       "      <td>67.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>768.0</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.24375</td>\n",
       "      <td>0.3725</td>\n",
       "      <td>0.62625</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>768.0</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>21.000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outcome</th>\n",
       "      <td>768.0</td>\n",
       "      <td>0.348958</td>\n",
       "      <td>0.476951</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count        mean         std     min       25%  \\\n",
       "Pregnancies               768.0    3.845052    3.369578   0.000   1.00000   \n",
       "Glucose                   768.0  120.894531   31.972618   0.000  99.00000   \n",
       "BloodPressure             768.0   69.105469   19.355807   0.000  62.00000   \n",
       "SkinThickness             768.0   20.536458   15.952218   0.000   0.00000   \n",
       "Insulin                   768.0   79.799479  115.244002   0.000   0.00000   \n",
       "BMI                       768.0   31.992578    7.884160   0.000  27.30000   \n",
       "DiabetesPedigreeFunction  768.0    0.471876    0.331329   0.078   0.24375   \n",
       "Age                       768.0   33.240885   11.760232  21.000  24.00000   \n",
       "Outcome                   768.0    0.348958    0.476951   0.000   0.00000   \n",
       "\n",
       "                               50%        75%     max  \n",
       "Pregnancies                 3.0000    6.00000   17.00  \n",
       "Glucose                   117.0000  140.25000  199.00  \n",
       "BloodPressure              72.0000   80.00000  122.00  \n",
       "SkinThickness              23.0000   32.00000   99.00  \n",
       "Insulin                    30.5000  127.25000  846.00  \n",
       "BMI                        32.0000   36.60000   67.10  \n",
       "DiabetesPedigreeFunction    0.3725    0.62625    2.42  \n",
       "Age                        29.0000   41.00000   81.00  \n",
       "Outcome                     0.0000    1.00000    1.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's use describe method to see if there is anything suspicious\n",
    "df.describe().T\n",
    "\n",
    "# NEEDS more cleaning than we do here because of the zeros that actually mean missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:18:04.372503Z",
     "start_time": "2019-10-30T20:18:04.364946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "Pregnancies                 768 non-null int64\n",
      "Glucose                     768 non-null int64\n",
      "BloodPressure               768 non-null int64\n",
      "SkinThickness               768 non-null int64\n",
      "Insulin                     768 non-null int64\n",
      "BMI                         768 non-null float64\n",
      "DiabetesPedigreeFunction    768 non-null float64\n",
      "Age                         768 non-null int64\n",
      "Outcome                     768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "## Now let's use info method\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:22:07.792507Z",
     "start_time": "2019-10-30T20:22:07.788548Z"
    }
   },
   "outputs": [],
   "source": [
    "## separate target variable from features\n",
    "target = df.Outcome\n",
    "df.drop('Outcome', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:22:08.479463Z",
     "start_time": "2019-10-30T20:22:08.475582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([500, 268]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's see the distribution of 1's and 0's\n",
    "np.unique(target, return_counts= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:22:34.319828Z",
     "start_time": "2019-10-30T20:22:34.308785Z"
    }
   },
   "outputs": [],
   "source": [
    "## Split data into test train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.20, stratify=target)\n",
    "# stratified the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this problem it makes sense to focus on recall score as we don't want to  misclassify patients with diabetes.\n",
    "\n",
    "Recall score = $\\frac{tp}{(tp + fn)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:24:38.909855Z",
     "start_time": "2019-10-30T20:24:38.903789Z"
    }
   },
   "outputs": [],
   "source": [
    "## First let's fit a logistic regression model to see the baseline\n",
    "## we will also use pipelines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## we will apply standard scaling to Logistic regression\n",
    "## because we might want to use regularization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:26:24.151220Z",
     "start_time": "2019-10-30T20:26:24.147559Z"
    }
   },
   "outputs": [],
   "source": [
    "## ll estimators in a pipeline, except the last one,\n",
    "## must be transformers (i.e. must have a transform method). \n",
    "## The last estimator may be any type (transformer, classifier, etc.)\n",
    "\n",
    "pipe = Pipeline([('ss', StandardScaler()), \n",
    "                 ('log_reg', LogisticRegression(random_state=123,\n",
    "                                                max_iter = 500, \n",
    "                                                solver = 'saga'))])\n",
    "\n",
    "# have to use standard scalar for regularization, because we are adding coefficients into cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:48:15.856709Z",
     "start_time": "2019-10-30T20:48:15.852640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ss', StandardScaler(copy=True, with_mean=True, with_std=True))\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=123, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "## we can access to a particular step in the pipeline\n",
    "\n",
    "print(pipe.steps[0])\n",
    "\n",
    "print(pipe['log_reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:43:26.956335Z",
     "start_time": "2019-10-30T20:43:26.945369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can call fit method with pipeline \n",
    "## we can call them with a gridsearch\n",
    "\n",
    "## let's use fit method from pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "## We can access the trained estimator from pipe\n",
    "pipe.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:43:34.462518Z",
     "start_time": "2019-10-30T20:43:34.143025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('ss',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('log_reg',\n",
       "                                        LogisticRegression(C=1.0,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=500,\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=123,\n",
       "                                                           solver='...\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'log_reg__C': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
       "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
       "       3.59381366e+01, 1.00000000e+02]),\n",
       "                          'log_reg__penalty': ['l1', 'l2']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to find a best value for the C\n",
    "## let's use GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = grid = [{'log_reg__C': np.logspace(-2,2,10, base = 10.0), \n",
    "                'log_reg__penalty': ['l1', 'l2']}]\n",
    "\n",
    "gridsearch = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=grid,\n",
    "                  scoring='recall', # check scikitlearn logistic regression and metrics documentation, could pass in list of scores?\n",
    "                  cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# use logspace when hyperperameter search for c, NOT linspace\n",
    "# WATCH OUT because this becomes exponentially costly when you add more things per grid line \n",
    "# try RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:54:30.031600Z",
     "start_time": "2019-10-30T20:54:29.704505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('ss',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('log_reg',\n",
       "                                        LogisticRegression(C=1.0,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=500,\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=123,\n",
       "                                                           solver='...\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'log_reg__C': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
       "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
       "       3.59381366e+01, 1.00000000e+02]),\n",
       "                          'log_reg__penalty': ['l1', 'l2']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to find a best value for the C\n",
    "## let's use GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = grid = [{'log_reg__C': np.logspace(-2,2,10, base = 10.0), \n",
    "                'log_reg__penalty': ['l1', 'l2'] #have to have \"log_reg__\" to set parameters for your log_reg\n",
    "               }]\n",
    "\n",
    "gridsearch = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=grid,\n",
    "                  scoring='recall', # check scikitlearn logistic regression and metrics documentation, could pass in list of scores?\n",
    "                  cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# use logspace when hyperperameter search for c, NOT linspace\n",
    "# WATCH OUT because this becomes exponentially costly when you add more things per grid line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T20:54:31.618683Z",
     "start_time": "2019-10-30T20:54:31.615263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best recall: 0.547\n",
      "\n",
      "Best params:\n",
      " {'log_reg__C': 0.5994842503189409, 'log_reg__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# Best recall\n",
    "print('Best recall: %.3f' % gridsearch.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Algorithms\n",
    "\n",
    "__Q:__ What is boosting?\n",
    "\n",
    " - Recall that random forest algorithm uses boosting aggregation (bagging) to decrease the variance of individual trees.\n",
    " - Boosting ~ Bagging \n",
    "      - Bagging: Trees grow parallel\n",
    "      - Boosting: Trees grow sequentially\n",
    " - Idea is to create a slow learner.\n",
    " \n",
    " `(bagging = bootstrapping aggregator)`\n",
    " \n",
    " `(biggest problem of decision tree is that they over-fit - form of variance)`\n",
    " \n",
    " `(random forest is like a president picking a cabinet of experts then going with majority vote)`\n",
    " \n",
    " `(boosting is i will learn slowly and learn from my mistakes; stump, it is an iterative method, runs in a process, so you cannot run things parallel)`\n",
    "\n",
    "Recall that in bagging we did bootstrapping in boosting we don't do bootstrapping instead we modify the dataset at each step.\n",
    "\n",
    "__important parameters__(with sklearn notation)\n",
    "\n",
    "__n_estimators:__ # of trees to use in the procedure\n",
    "\n",
    "\n",
    "__learning_rate:__ (Shrinkage parameter)\n",
    "\n",
    "> The shrinkage parameter $\\lambda$, a small positivenumber.This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small  $\\lambda$ can require using a very large value of B in order to achieve good performance\n",
    "\n",
    "\n",
    "<img src=\"img/boosting_algorithm.png\" width=450, height=450> \n",
    "\n",
    "\n",
    "lambda = learning_rate (between 0-1), we will try to change, but there is a tradeoff, if you decrease learning rate you should put more trees to be able to learn\n",
    "\n",
    "sub sampling with learning rate\n",
    "\n",
    "__max_depth, max_leaf_nodes etc,__ (The number of splits in each trees)\n",
    "\n",
    "> Often d = 1 works well, in which case each tree is called a _stump_, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T21:27:07.081538Z",
     "start_time": "2019-10-30T21:27:07.073282Z"
    }
   },
   "outputs": [],
   "source": [
    "## Now let's investigate the performance of Adaboost and GradientBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "## let's see some of the parameters of the Gradient Boosting\n",
    "?GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T21:27:07.962156Z",
     "start_time": "2019-10-30T21:27:07.959316Z"
    }
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state= 103019,\n",
    "                                 validation_fraction=0.1, \n",
    "                                 n_iter_no_change= 5, \n",
    "                                 tol = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T21:27:34.052766Z",
     "start_time": "2019-10-30T21:27:10.047056Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1403 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed:   24.0s finished\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='deviance', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=100,\n",
       "                                                  n_iter_no...\n",
       "                                                  subsample=1.0, tol=0.005,\n",
       "                                                  validation_fraction=0.1,\n",
       "                                                  verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': array([0.001     , 0.00316228, 0.01      , 0.03162278, 0.1       ]),\n",
       "                         'max_features': [0.5, 1],\n",
       "                         'max_leaf_nodes': [3, 5, 7, 9],\n",
       "                         'n_estimators': [100, 200, 300],\n",
       "                         'subsample': [0.2, 0.5, 0.7, 0.9]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's use a gridsearch to find best parameters for GradientBoost\n",
    "\n",
    "params = {'n_estimators' : [100, 200, 300],\n",
    "         'learning_rate' : np.logspace(-3, -1, 5),\n",
    "         'max_leaf_nodes': [3,5,7,9],\n",
    "         'subsample': [0.2, 0.5, 0.7, 0.9], \n",
    "         'max_features':[0.5,1]}\n",
    "\n",
    "gs = GridSearchCV(estimator = gbc, \n",
    "                  param_grid = params,\n",
    "                  cv = 5, \n",
    "                  scoring= 'recall',\n",
    "                  verbose = 1,\n",
    "                  n_jobs= -1)\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some practical tips for Gradient Boost - TO IMPROVE YOUR TIMING\n",
    "\n",
    "- Apparently max_leaf_nodes = k gives similar results to max_depth = k-1 but according to sklearn documentation **max_leaf_nodes works faster**. So you might want to use max_leaf_nodes for bigger projects.\n",
    "\n",
    "- Again according to sklearn documentation, smaller learning rate gives better test_scores but you might want to put more estimators if you set the learning rate small.\n",
    "\n",
    "- As it is mentioned above, when small learning rate is used we might increase the number of estimators. To prevent unneccesarry computing then we can put some early stopping criteria by the parameters: n_iter_change, min_impurity_decrease or tol.\n",
    "\n",
    "- It looks like subsampling with shrinkage method (learning rate) might give better results. In this case, out of bag test scoring is also become available. Note that you can access these by oob_improvement method.\n",
    "\n",
    "- Using a small max_features value can significantly decrease the runtime.\n",
    "\n",
    "For more: \n",
    "[sklearn documentation - gradientboost](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T21:27:46.423007Z",
     "start_time": "2019-10-30T21:27:46.419002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5514625515383034\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=0.5, max_leaf_nodes=9,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=5, presort='auto',\n",
      "                           random_state=103019, subsample=0.7, tol=0.005,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T21:28:02.041033Z",
     "start_time": "2019-10-30T21:28:02.031103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5370370370370371\n",
      "0.5740740740740741\n"
     ]
    }
   ],
   "source": [
    "## let's see the best_estimator's test performance\n",
    "best_estimator = gs.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "\n",
    "## import recall_score from sklearn\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(recall_score(y_test, y_pred))\n",
    "\n",
    "## similarly log_reg predictor would give\n",
    "\n",
    "log_reg_best = gridsearch.best_estimator_\n",
    "y_pred_log = log_reg_best.predict(X_test)\n",
    "\n",
    "print(recall_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T21:31:40.886325Z",
     "start_time": "2019-10-30T21:31:40.879989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5700934579439252\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_estimator.predict(X_train)\n",
    "\n",
    "print(recall_score(y_train, y_train_pred))\n",
    "\n",
    "## try the same thing with log_reg_best: Do you expect better score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try Adaboost algorithm and XGboost here\n",
    "## Use gridsearch or RandomSearchCV to fine-tune parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
